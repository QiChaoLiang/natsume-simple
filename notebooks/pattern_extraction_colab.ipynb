{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbAkp7SgKQnP",
    "tags": []
   },
   "source": [
    "# 検索データ作成\n",
    "\n",
    "[Natsume](https://hinoki-project.org/natsume/)は，名詞ー格助詞ー動詞などの構文パターンを検索したり，そのジャンル間の使用を比較したりすることができるシステムです。\n",
    "ここでは，その検索機能の位置，名詞ー格助詞ー動詞の構文パターンを抽出することにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwnTJ5lcKQnR",
    "tags": []
   },
   "source": [
    "## 準備\n",
    "\n",
    "すでに`requirements.txt`を`pip`などでインストール済みだったら，以下の宣言は不要です。Google Colabなどの場合は実行が必要になります（`#`をとってください）。\n",
    "\n",
    "DHラボのiMacはここで`pip3`を使ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jad2xhvhKQnR",
    "outputId": "8e68a816-ffe5-4a8b-9010-f1e41aecffd0"
   },
   "outputs": [],
   "source": [
    "![[ ! -d natsume-simple ]] && git clone https://github.com/borh/natsume-simple.git\n",
    "![[ ! -d natsume-simple ]] && mv natsume-simple/* .\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmkgxks8KQnS",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ginza\n",
    "\n",
    "Ginza は SpaCy を裏で使っているので，SpaCy と使用がほとんど変わりません。ただし，一部追加機能があります。\n",
    "追加機能は主に文節処理とトーケン（形態素）のレマ（語彙素）参照です。詳しくは[公式サイトへ](https://megagonlabs.github.io/ginza/)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgY1U1zPKQnS",
    "outputId": "c7539477-ba4f-4719-ff33-db25f6c66677"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import ginza\n",
    "\n",
    "try:\n",
    "    is_using_gpu = spacy.prefer_gpu()  # GPUがあれば，使う # GPU搭載ならここでコメントを外す\n",
    "except:\n",
    "    is_using_gpu = False\n",
    "\n",
    "if is_using_gpu:\n",
    "    print(\"Using GPU\")\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"ja_ginza_electra\")  # あればja_ginza_electraを使用\n",
    "    model_name = \"ja_ginza_electra\"\n",
    "except Exception:\n",
    "    nlp = spacy.load(\"ja_ginza\")  # なけらばja_ginza\n",
    "    model_name = \"ja_ginza\"\n",
    "\n",
    "ginza.force_using_normalized_form_as_lemma(True)\n",
    "\n",
    "example_sentence = \"東京では，銀座でランチを食べよう。\"\n",
    "doc = nlp(example_sentence)\n",
    "[\n",
    "    (\n",
    "        token.i,\n",
    "        token.orth_,\n",
    "        token.lemma_,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.dep_,\n",
    "        token.head.i,\n",
    "        ginza.inflection(token),\n",
    "    )\n",
    "    for token in doc\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_r3BmDiKQnT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 係り受けの例\n",
    "\n",
    "今回対象としている名詞ー格助詞ー動詞（NPV）パターンは係り受け構造の中で，どのように現れるか，簡単な例で示せます。\n",
    "SpaCy/Ginza で使用される係り受け構造の定義は Universal Dependencies 2 をご参照ください。\n",
    "\n",
    "上記の例でもわかるように，名詞ー格助詞ー動詞でナイーブな抽出を行うと，「東京で食べる」「銀座で食べる」「ランチを食べる」の３共起表現が抽出されます。ただし，その中の「東京で食べる」は実は「で」格助詞単独ではなく，「では」という連語の形で出現します。\n",
    "goo 辞書の解説では[以下の通り](https://dictionary.goo.ne.jp/word/%E3%81%A7%E3%81%AF/)定義されています：\n",
    "\n",
    "```\n",
    " で‐は の解説\n",
    "\n",
    "［連語］\n",
    "《断定の助動詞「だ」の連用形＋係助詞「は」》判断の前提を表す。…であるとすれば。…だと。「雨では中止になる」「彼ではだれも承知しないだろう」\n",
    "《格助詞「で」＋係助詞「は」》…で。…においては。…を用いては。「今日では問題にされない」\n",
    "《接続助詞「で」＋係助詞「は」》未然形に付く。\n",
    "[...]\n",
    "```\n",
    "\n",
    "他にも「でも」「へと」「へは」「からは」など複合助詞が存在し，単独係助詞よりは文法的な役割が複雑なため，検索対象から外すようにします。\n",
    "\n",
    "### 係り受け関係の可視化\n",
    "\n",
    "Cabocha や KNP のように文節を係り受けの単位にしているものと違い，SpaCy/GiNZA ではトーケン（形態素）を単位として係り受け関係を表しています。\n",
    "そのため，長文になればなるほど，その構造が最初の例（print 関数などを使う）より読みにくくなってしまいます。\n",
    "そのため，SpaCy では可視化ツール displacy を用意しています。\n",
    "\n",
    "よく使うので，最初にヘルパー関数 pp を定義し，文字列を入力として簡単にかかり受け図を出力するようにしておきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "EHVAax1QKQnT",
    "outputId": "4c48aece-5d72-44db-8f16-339537374cd3"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# https://spacy.io/api/top-level#displacy_options\n",
    "compact = {\n",
    "    \"compact\": True,\n",
    "    \"add_lemma\": True,\n",
    "    \"distance\": 100,\n",
    "    \"word_spacing\": 30,\n",
    "    \"color\": \"#ffffff\",\n",
    "    \"bg\": \"#1e1e1e\",  # dark modeでない場合は，コメントアウト\n",
    "}  # 表示を長い文用に工夫\n",
    "\n",
    "\n",
    "def pp(s: str):\n",
    "    return displacy.render(nlp(s), options=compact, jupyter=True)\n",
    "\n",
    "\n",
    "pp(\"東京では，銀座でランチを食べよう。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "__c4V2edKQnT",
    "outputId": "c8f305e0-d201-40f8-efde-86335cf5b776"
   },
   "outputs": [],
   "source": [
    "# 残念ながら，y幅は調整できない\n",
    "pp(\n",
    "    \"１８紀の哲学者ヒュームは，「力はいつも被治者の側にあり，支配者には自分たちを支えるものは世論以外に何もないということがわかるであろう」と論じているが，仮に選挙がなくとも，大多数の被治者からの暗黙の同意がなければ如何なる政治体制も不安定にならざるを得ないだろう。\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "MofY8FW4KQnU",
    "outputId": "ae8c194e-8b18-4a6a-a24c-b5be22068f43"
   },
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"さらに，数年おきに選挙が行われるだけではなく，マスメディアが発達し，世論調査が頻繁に行われている現在の状況を考えれば，以前と比べて，民意の重要性は，高まっていると思われる。\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jUmhRYCKQnU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 係り受け関係の抽出\n",
    "\n",
    "係り受け関係は SpaCy で DependencyMatcher という機能で検索できます。\n",
    "\n",
    "- <https://spacy.io/usage/rule-based-matching#dependencymatcher>\n",
    "\n",
    "Semgrex の記号を使うことによって，係り受け構造の定義がわりと自由にできます。\n",
    "\n",
    "```\n",
    "SYMBOL\tDESCRIPTION\n",
    "A < B\tA is the immediate dependent of B.\n",
    "A > B\tA is the immediate head of B.\n",
    "A << B\tA is the dependent in a chain to B following dep → head paths.\n",
    "A >> B\tA is the head in a chain to B following head → dep paths.\n",
    "A . B\tA immediately precedes B, i.e. A.i == B.i - 1, and both are within the same dependency tree.\n",
    "A .* B\tA precedes B, i.e. A.i < B.i, and both are within the same dependency tree (not in Semgrex).\n",
    "A ; B\tA immediately follows B, i.e. A.i == B.i + 1, and both are within the same dependency tree (not in Semgrex).\n",
    "A ;* B\tA follows B, i.e. A.i > B.i, and both are within the same dependency tree (not in Semgrex).\n",
    "A $+ B\tB is a right immediate sibling of A, i.e. A and B have the same parent and A.i == B.i - 1.\n",
    "A $- B\tB is a left immediate sibling of A, i.e. A and B have the same parent and A.i == B.i + 1.\n",
    "A $++ B\tB is a right sibling of A, i.e. A and B have the same parent and A.i < B.i.\n",
    "A $-- B\tB is a left sibling of A, i.e. A and B have the same parent and A.i > B.i.\n",
    "```\n",
    "\n",
    "DependencyMatcher の利用が向いているのは，検索対象の型が固定であり，マッチングに否定が必要ない時です。\n",
    "しかし，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOJY3M7vKQnU"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu9wJKIWKQnU",
    "outputId": "e46fa739-1113-4b5e-daa2-a96c9f46e82a"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "def token(name, attrs, dep_name=None, rel_op=None):\n",
    "    spec = {\n",
    "        \"RIGHT_ID\": name,\n",
    "        \"RIGHT_ATTRS\": attrs,\n",
    "    }\n",
    "    if dep_name and rel_op:\n",
    "        spec[\"LEFT_ID\"] = dep_name\n",
    "        spec[\"REL_OP\"] = rel_op\n",
    "    return spec\n",
    "\n",
    "\n",
    "token(\"verb\", {\"POS\": \"VERB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I43037yHKQnU",
    "outputId": "10512c39-5451-4669-c3e8-74d1bd2682c0"
   },
   "outputs": [],
   "source": [
    "token(\"noun\", {\"DEP\": {\"IN\": [\"obj\", \"obl\", \"nsubj\"]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a88gpkQ5KQnV",
    "outputId": "3173ca24-fc73-4d27-ddb6-398817fff9bd"
   },
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    # anchor token: VERB\n",
    "    {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "    #\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"noun\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"obl\", \"nsubj\"]}},\n",
    "    },\n",
    "    #\n",
    "    {\n",
    "        \"LEFT_ID\": \"noun\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"case_particle\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"case\",\n",
    "            \"LEMMA\": {\"IN\": [\"が\", \"を\", \"に\", \"で\", \"から\", \"より\", \"と\", \"へ\"]},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    "def matches_to_npv(doc: Doc, matches):\n",
    "    exclude_matches: set[int] = set()\n",
    "    for match_id, (verb, noun, case_particle) in matches:\n",
    "        if doc[case_particle + 1].pos_ == \"ADP\":\n",
    "            print(\n",
    "                \"Double particle:\", doc[case_particle : case_particle + 2], \"excluding.\"\n",
    "            )\n",
    "            exclude_matches.add(match_id)\n",
    "    matches = [m for m in matches if m[0] not in exclude_matches]\n",
    "    return matches\n",
    "\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"NPV\", [pattern])\n",
    "matches = matcher(doc)\n",
    "matches = matches_to_npv(doc, matches)\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVzrlqPUKQnV"
   },
   "outputs": [],
   "source": [
    "# https://github.com/explosion/spaCy/blob/master/spacy/symbols.pyx\n",
    "# GiNZA特有のシンボルcaseがないことに注意。文字列ではなく以下のようにsymbolを使うことで処理が若干早くなる。\n",
    "from typing import Iterable\n",
    "from spacy.symbols import (\n",
    "    NOUN,\n",
    "    PROPN,\n",
    "    PRON,\n",
    "    NUM,\n",
    "    VERB,\n",
    "    SYM,\n",
    "    PUNCT,\n",
    "    ADP,\n",
    "    SCONJ,\n",
    "    obj,\n",
    "    obl,\n",
    "    nsubj,\n",
    ")\n",
    "from spacy.tokens import Token, Span\n",
    "from ginza import bunsetu_span, inflection\n",
    "import ginza  # 他のメソッドなどを使う時\n",
    "from itertools import takewhile, tee  # pairwiseがPython 3.10で登場\n",
    "import re\n",
    "\n",
    "from collections.abc import Iterator\n",
    "\n",
    "\n",
    "def pairwise(iterable: Iterable[Any]):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "\n",
    "def simple_lemma(token: Token) -> str:\n",
    "    if token.lemma_ == \"為る\":\n",
    "        return \"する\"\n",
    "    elif token.lemma_ == \"居る\":\n",
    "        return token.orth_\n",
    "    elif token.lemma_ == \"成る\":\n",
    "        return token.orth_\n",
    "    elif token.lemma_ == \"有る\":\n",
    "        return token.orth_\n",
    "    else:\n",
    "        return token.lemma_\n",
    "\n",
    "\n",
    "def normalize_verb_span(tokens):\n",
    "    \"\"\"動詞が入っている文節のトーケンを入力として，正規化された動詞の文字列を返す。\n",
    "    現在「ます」「た」は除外とし，基本形に直す処理をしているが，完全にすべての活用の組み合わせに対応していな。\"\"\"\n",
    "    clean_tokens = [\n",
    "        token for token in tokens if token.pos not in {PUNCT, SYM}\n",
    "    ]  # 。「」などが始め，途中，終わりに出現することがあるので除外\n",
    "    clean_tokens = list(\n",
    "        takewhile(\n",
    "            lambda token: token.pos not in {ADP, SCONJ}\n",
    "            and token.lemma_ not in {\"から\", \"ため\", \"たり\", \"こと\", \"よう\"},\n",
    "            clean_tokens,\n",
    "        )\n",
    "    )  # いる>>と(ADP)<<いう，「いる>>から(SCONJ)<<」は品詞で除外すると「て」も除外される\n",
    "    if len(clean_tokens) == 1:\n",
    "        return simple_lemma(clean_tokens[0])\n",
    "\n",
    "    normalized_tokens: list[Token] = []\n",
    "    token_pairs: list[tuple[Token, Token]] = list(pairwise(clean_tokens))\n",
    "    for i, (token, next_token) in enumerate(token_pairs):\n",
    "        normalized_tokens.append(token)\n",
    "        if next_token.lemma_ == \"ます\" or next_token.lemma_ == \"た\":\n",
    "            if re.match(r\"^(五|上|下|サ|.変格|助動詞).+\", inflection(token)):\n",
    "                # TODO: ませんでした\n",
    "                break\n",
    "            else:\n",
    "                normalized_tokens.append(nlp(\"する\")[0])\n",
    "                break\n",
    "        elif next_token.lemma_ == \"だ\":  # なら(ば)，説明する>>なら(lemma=だ)<<，\n",
    "            break\n",
    "        elif i == len(token_pairs) - 1:  # ペアが最後の場合はnext_tokenも格納\n",
    "            normalized_tokens.append(next_token)\n",
    "\n",
    "    if len(normalized_tokens) == 1:\n",
    "        return simple_lemma(normalized_tokens[0])\n",
    "\n",
    "    if not normalized_tokens:\n",
    "        return None\n",
    "\n",
    "    stem = normalized_tokens[0]\n",
    "    affixes = normalized_tokens[1:-1]\n",
    "    suffix = normalized_tokens[-1]\n",
    "    return \"{}{}{}\".format(\n",
    "        stem.text,  # .lemma_を使う場合は未然形・連用形など注意する必要あり\n",
    "        \"\".join(t.text for t in affixes),\n",
    "        simple_lemma(suffix),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1b8VyVVKQnV",
    "outputId": "7aeb9707-10b2-4045-a4a3-57f822720001"
   },
   "outputs": [],
   "source": [
    "[inflection(t) for t in nlp(\"語ります\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sMYqPhKKQnW"
   },
   "source": [
    "### テストの活用\n",
    "\n",
    "自然言語はその単語の組み合わせが膨大で，すべてをルールで記載するつもりが例外出てきて思わぬ結果になることが多いです。\n",
    "ルールあるいはプログラムのアルゴリズム・処理などを検証しながら開発を進みたいときは，Python のテスト機能を活用とよいでしょう。\n",
    "しばしば，ノートブックのセルでの実行結果を見ながら書くよりはテストにおさめて，いかなる変更で，以前できた処理ができなかったりする場合やほしい結果がどの時点で得られたかを早期発見できます。\n",
    "\n",
    "以下では，全箇所が正しく処理されるのに対し，最後は失敗します。\n",
    "（実際は「見られない」が正しいですが，失敗の例として「見られないが」を正解にしています。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWUhYCVTKQnW",
    "outputId": "e9131836-e6cd-423c-9fb7-d5ecca299722"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestVerbNormalization(unittest.TestCase):\n",
    "    def test_norm(self):\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"いるからで\")), \"いる\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"いるという\")), \"いる\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"語ります\")), \"語る\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"しました。\")), \"する\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"作り上げたか\")), \"作り上げる\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"見られなかったが\")), \"見られないが\")\n",
    "\n",
    "# ノートブックの中では以下のようにユニットテストできる：\n",
    "unittest.main(\n",
    "    argv=[\"ignored\", \"-v\", \"TestVerbNormalization.test_norm\"], verbosity=2, exit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9m9TzbW-KQnW",
    "outputId": "34630502-fe9e-4949-eae7-4614e15c392a"
   },
   "outputs": [],
   "source": [
    "# https://megagonlabs.github.io/ginza/bunsetu_api.html\n",
    "\n",
    "\n",
    "def npv_matcher(doc: Doc):\n",
    "    matches: list[tuple[str, str, str]] = []\n",
    "    for token in doc[:-2]:  # 検索対象の最小トーケン数が３のため，最後の2トーケンは見なくて良い\n",
    "        noun = token\n",
    "        case_particle = noun.nbor(1)\n",
    "        verb = token.head\n",
    "        if (\n",
    "            noun.pos in {NOUN, PROPN, PRON, NUM}\n",
    "            and noun.dep in {obj, obl, nsubj}\n",
    "            and verb.pos == VERB\n",
    "            and case_particle.dep_ == \"case\"\n",
    "            and case_particle.lemma_ in {\"が\", \"を\", \"に\", \"で\", \"から\", \"より\", \"と\", \"へ\"}\n",
    "            and case_particle.nbor().dep_ != \"fixed\"\n",
    "            and case_particle.nbor().head != case_particle.head\n",
    "        ):  # では，には，をも，へとなどを除外\n",
    "            verb_bunsetu_span = bunsetu_span(verb)\n",
    "            vp_string = normalize_verb_span(verb_bunsetu_span)\n",
    "            if not vp_string:\n",
    "                print(\n",
    "                    \"Error normalizing verb phrase:\",\n",
    "                    verb_bunsetu_span,\n",
    "                    \"in document\",\n",
    "                    doc,\n",
    "                )\n",
    "                continue\n",
    "            matches.append(\n",
    "                (\n",
    "                    noun.lemma_,\n",
    "                    case_particle.lemma_,\n",
    "                    # verb.lemma_,\n",
    "                    vp_string,\n",
    "                )\n",
    "            )\n",
    "    return matches\n",
    "\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestExtraction(unittest.TestCase):\n",
    "    def test_npv(self):\n",
    "        self.assertEqual(\n",
    "            npv_matcher(nlp(example_sentence)),\n",
    "            [(\"銀座\", \"で\", \"食べる\"), (\"ランチ\", \"を\", \"食べる\")],\n",
    "        )\n",
    "        self.assertEqual(npv_matcher(nlp(\"京都にも行く。\")), [])\n",
    "        self.assertEqual(npv_matcher(nlp(\"ことを説明するならば\")), [(\"こと\", \"を\", \"説明する\")])\n",
    "        # ここは「ことになる」あるいは「ことにならない」が正しいが，GiNZAではこれがイディオム処理（fixed/compound）のため，「ざるをえない」などと一緒に処理すべき\n",
    "        self.assertEqual(npv_matcher(nlp(\"ことにならない\")), [(\"こと\", \"に\", \"ならない\")])\n",
    "\n",
    "\n",
    "unittest.main(\n",
    "    argv=[\"ignored\", \"-v\", \"TestExtraction.test_npv\"], verbosity=2, exit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqyrz__PKQnW",
    "outputId": "6eae69a0-3e6a-4561-c5a6-eba277bb1f5d"
   },
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"彼が語ります\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYsvehcUKQnX",
    "outputId": "c93cb830-922e-4385-a4a7-985c6968d445"
   },
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"ことを説明するならば\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSpHyA19KQnX",
    "outputId": "6c755d3d-9955-42c4-fdff-434cb9a43e45"
   },
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"ことにならない\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKUc7ITMKQnX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TED トークコーパスの作成\n",
    "\n",
    "Hugginface の datasets を使って，TED トークの日本語に翻訳された字幕をコーパス化します。\n",
    "データセットのページは以下：\n",
    "\n",
    "- <https://huggingface.co/datasets/ted_talks_iwslt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "a810316177ff4250bba73158cd9de5a1",
      "ae4a7cba9e574d03896f0f5faadcbba0",
      "f86ae8ea86704fcaa34156e83ef78ade",
      "7139ca4c7d7942449e6e3d2ebeb1a754",
      "9e53fca632e64e8f97f4f0f3087cfdae",
      "282eaa9b72434a86a1b2aec03d112fd3",
      "8b535a5f82c745dea806b5181c721c83",
      "9f58a3baae4d426283ece6599216333d",
      "e2e26054b7c54d70b83a7e195e4e13c7",
      "d5c01a639e4c41be81b957d76a25e67a",
      "67b1999627214e0ca87541278095e1a0",
      "941e59faa98a43a8a59bc4d40d5938cb",
      "ed3b5e160b8840af883bb7fcdaa466a8",
      "cbb88c222d9146e7a7b42744a2b90550",
      "ccf9fe9cb514466794ae66ad257187db",
      "1cfd1c889c454e3fa96d249216ed1b65",
      "f80581dbf72d4ba7a8f85022751a13f1",
      "ab2dc17ec54f43b39c256245fcda8d99",
      "0a5eda906e77403e9263328be4d916c5",
      "e19b1f6e31df48238a86c2c5c60541e2",
      "2e1a13abd564409589315a341daa3e34",
      "c63ef4e03ba249bb8321c347c305bc2f",
      "3eec3d8d141649d796d1b0eb42440420",
      "a10724c299ad4fe3a0092e740f16757c",
      "fb35b6b6371041dcb3d11228672745ea",
      "962cc4f2125d4b24aa559b6fda1e2cf6",
      "dd3f5221284048f0b50663c122bb29a1",
      "8d7898fd8b1c43909ae11139dc69a888",
      "543b02e2ced64361914072bb16465e08",
      "d2e19a2c30894da7bfd12f22de5a29c3",
      "8575e6e122414ec1b7b5a2707d6ced14",
      "77cba3284fd1412e9a5c6552f615ae55",
      "c4b2e08ab7714120add70b37b0ff4d79"
     ]
    },
    "id": "r5PIoCdfKQnX",
    "outputId": "e7a1add3-748d-4097-dacd-e3f74405b167"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ted_dataset_2014 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2014\"\n",
    ")\n",
    "ted_dataset_2015 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2015\"\n",
    ")\n",
    "ted_dataset_2016 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2016\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xt4CoOxKQnX",
    "outputId": "8783f0d3-669a-41c3-a8e5-577430200550"
   },
   "outputs": [],
   "source": [
    "ted_corpus = (\n",
    "    [d[\"ja\"] for d in ted_dataset_2014[\"train\"][\"translation\"]]\n",
    "    + [d[\"ja\"] for d in ted_dataset_2015[\"train\"][\"translation\"]]\n",
    "    + [d[\"ja\"] for d in ted_dataset_2016[\"train\"][\"translation\"]]\n",
    ")\n",
    "len(ted_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LSuOkLdKQnY",
    "outputId": "1186c8d3-701c-45d1-9039-554cd202bb2d"
   },
   "outputs": [],
   "source": [
    "ted_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QdWtg-3KQnY"
   },
   "outputs": [],
   "source": [
    "with open(\"./data/ted_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(ted_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "Itjynec2KQnY",
    "outputId": "716918b5-7aec-43d3-b3f0-437dbed631e1"
   },
   "outputs": [],
   "source": [
    "pp(\"トビー・エクルズは、この状況を覆すための画期的なアイデア「ソーシャル・インパクト・ボンド（社会インパクト債権）」について話します。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "EiMkSsq5KQnY",
    "outputId": "bf99c98f-78fd-4630-fcf3-6c0dd12f15b2"
   },
   "outputs": [],
   "source": [
    "pp(\"野生生物の保護に尽力するボイド・ヴァーティは「自然の大聖堂は人間性の最高の部分を映し出してくれる鏡である」と話します。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "pKJSs1mzKQnY",
    "outputId": "bcb95c28-e2e9-4ead-e448-b618075b6da3"
   },
   "outputs": [],
   "source": [
    "pp(\"チャンの素晴らしい手作りの弓がしなる様子をご堪能ください。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "U86GEoBUKQnY",
    "outputId": "f04a70a4-0a90-4edb-e185-77de01ee3dfa"
   },
   "outputs": [],
   "source": [
    "pp(\"アメリカ人が「共有する」市民生活は、どれだけお金を持っているかによって違うものになってしまったと言っていいでしょう。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "EVWNnA3VKQnY",
    "outputId": "efae411a-1841-4186-e6df-75b238f4518a"
   },
   "outputs": [],
   "source": [
    "pp(\"しかし飛行機や自動車が生まれて100年がたった今も、それが本当に実現されたことはありませんでした。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "_oehFsF1KQnY",
    "outputId": "5bf290ac-4414-4385-cf97-d41f3fe7f93f"
   },
   "outputs": [],
   "source": [
    "# バグ\n",
    "pp(\"TEDxTC でジョナサン・フォーリーが「テラカルチャー」（地球全体のための農業）に取り組む必要性を訴えます。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "pniuKus5KQnZ",
    "outputId": "cdd3fc54-8b6e-4115-ac4c-3926eb54e3e4"
   },
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrFnufZ7KQnZ",
    "outputId": "caa67112-124c-45a0-c72d-e9713d9adb64"
   },
   "outputs": [],
   "source": [
    "ginza.bunsetu_spans(\n",
    "    nlp(\n",
    "        \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl0tI7H0KQnZ",
    "outputId": "308be8a5-ab53-4296-8480-b3902c427241"
   },
   "outputs": [],
   "source": [
    "npv_matcher(\n",
    "    nlp(\n",
    "        \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoSNZSuSKQna",
    "tags": []
   },
   "source": [
    "## コーパスからの抽出処理\n",
    "\n",
    "処理する文章が多いときは`nlp.pipe()`を使い，文字列のリストを引数にすることで，並列処理が行えます。\n",
    "そこから得られた doc(s) を npv_matcher に渡し，chain.from_iterable でくっつけます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50mkkxkSKQna"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "ted_npvs = list(chain.from_iterable(npv_matcher(doc) for doc in nlp.pipe(ted_corpus)))\n",
    "# GPU 25s   (ja_ginza) / 1m3s  (ja_ginza_electra) (A4000)\n",
    "# CPU 1m31s (ja_ginza) / 5m42s (ja_ginza_electra) (3960x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWiOvD3KKQna",
    "outputId": "84ca5f99-cf64-45af-9faa-ec1029247ce0"
   },
   "outputs": [],
   "source": [
    "len(ted_npvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTPbu_GeKQna",
    "outputId": "3dec960e-eecd-403e-8ce7-b4642397e4e0"
   },
   "outputs": [],
   "source": [
    "# 格助詞ごとの項目数を調べるなら\n",
    "from collections import Counter\n",
    "\n",
    "Counter(npv[1] for npv in ted_npvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OXq-XbPKQna"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtnaNjNiKQna",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NPV データの保存\n",
    "\n",
    "検索インターフェースでは今回 NPV パターンのみを検索するため，そのデータのみを CSV 形式に書き出す。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56xVrAkmKQna",
    "outputId": "a41cb5c8-bdf3-418b-deda-c290dfb2ec1e"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./data/\")\n",
    "data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Q7Faxm_uKQna",
    "outputId": "bb0da060-c428-4169-ee60-9c92e6cb46f8"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(ted_npvs, columns=[\"n\", \"p\", \"v\"])\n",
    "df[\"corpus\"] = \"TED\"\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5HslBzlKQnb"
   },
   "outputs": [],
   "source": [
    "df.to_csv(data_dir / f\"ted_npvs_{model_name}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReeqM0yyRUjW",
    "outputId": "7e4dbafd-422c-4679-c832-2e9a4e8e64ca"
   },
   "outputs": [],
   "source": [
    "# Google Colab has an old version of Pandoc, so we download and install the latest release.\n",
    "!wget -c https://github.com/jgm/pandoc/releases/download/2.19.2/pandoc-2.19.2-1-amd64.deb\n",
    "!sudo dpkg -i pandoc-2.19.2-1-amd64.deb\n",
    "# Using this Pandoc, the following scripts should work: (Bash and Python versions are interchangeable)\n",
    "!cd scripts && ./get-jnlp-corpus.sh\n",
    "!cd scripts && python ./convert-jnlp-corpus.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RraC0ynbKQnb"
   },
   "outputs": [],
   "source": [
    "with open(data_dir / \"jnlp-sample-3000-python.txt\", encoding=\"utf-8\") as f:\n",
    "    jnlp_corpus = f.readlines()\n",
    "\n",
    "jnlp_npvs = list(chain.from_iterable(npv_matcher(doc) for doc in nlp.pipe(jnlp_corpus, disable=[\"ner\"], batch_size=100, n_process=2)))\n",
    "# GPU 28s   (ja_ginza) / 1m6s (ja_ginza_electra) (A4000)\n",
    "# CPU 1m42s (ja_ginza) / 6m2s (ja_ginza_electra) (3960x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tGgG0HimKQnb",
    "outputId": "c54f70f0-1b5a-4102-a43a-048674ee16fc"
   },
   "outputs": [],
   "source": [
    "j_df = pd.DataFrame.from_records(jnlp_npvs, columns=[\"n\", \"p\", \"v\"])\n",
    "j_df[\"corpus\"] = \"自然言語処理\"\n",
    "j_df.to_csv(f\"./data/jnlp_npvs_{model_name}.csv\", index=False)\n",
    "j_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "gcvAnpgdKQnb",
    "outputId": "59124cde-2f56-4bcd-c9c0-87c19c98f237"
   },
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"共参照関係認定基準1を用いた場合と共参照関係認定基準2を用いた場合とを比較すると，共参照関係認定基準2の方が厳しい制約であるため，再現率が低下するかわりに，適合率が上昇している．\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLLGbjt9KQnb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('natsume-simple-fwSDaNdN-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "051b93bfe2e1f91d147f006f9b5e502299c33a19db14a2b4cc48dee3500e361a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
